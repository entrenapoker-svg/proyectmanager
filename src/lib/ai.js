
import { GoogleGenerativeAI } from "@google/generative-ai";

// --- GROQ API HELPER ---
const generateGroqResponse = async (apiKey, modelName, messages) => {
    try {
        const response = await fetch("https://api.groq.com/openai/v1/chat/completions", {
            method: "POST",
            headers: {
                "Authorization": `Bearer ${apiKey}`,
                "Content-Type": "application/json"
            },
            body: JSON.stringify({
                messages: messages,
                model: modelName,
                temperature: 0.7
            })
        });

        if (!response.ok) {
            const errorData = await response.json();
            throw new Error(`Groq API Error: ${errorData.error?.message || response.statusText}`);
        }

        const data = await response.json();
        return data.choices[0]?.message?.content || "";
    } catch (error) {
        throw error;
    }
};

// --- HUGGING FACE API HELPER ---
const generateHuggingFaceResponse = async (apiKey, modelName, messages) => {
    try {
        // HF Inference API usually takes a simple prompt string for some models, or messages for chat models.
        // We will use the v1/chat/completions compatible endpoint if available, but standard Inference API is more reliable for free tier.
        // Let's use the standard Inference API with a simple prompt construction.

        const systemMsg = messages.find(m => m.role === 'system')?.content || "";
        const userMsg = messages.find(m => m.role === 'user')?.content || "";
        const fullPrompt = `<|system|>\n${systemMsg}\n<|user|>\n${userMsg}\n<|assistant|>\n`;

        const response = await fetch(`https://api-inference.huggingface.co/models/${modelName}`, {
            method: "POST",
            headers: {
                "Authorization": `Bearer ${apiKey}`,
                "Content-Type": "application/json"
            },
            body: JSON.stringify({
                inputs: fullPrompt,
                parameters: {
                    max_new_tokens: 1024,
                    temperature: 0.7,
                    return_full_text: false
                }
            })
        });

        if (!response.ok) {
            const errorText = await response.text();
            throw new Error(`HuggingFace API Error: ${errorText}`);
        }

        const data = await response.json();
        // HF returns array of objects, usually [{ generated_text: "..." }]
        return Array.isArray(data) ? data[0]?.generated_text : data?.generated_text || "";
    } catch (error) {
        throw error;
    }
};

// --- COHERE API HELPER ---
const generateCohereResponse = async (apiKey, modelName, messages) => {
    try {
        const systemMsg = messages.find(m => m.role === 'system')?.content || "";
        const userMsg = messages.find(m => m.role === 'user')?.content || "";

        const response = await fetch("https://api.cohere.ai/v1/chat", {
            method: "POST",
            headers: {
                "Authorization": `Bearer ${apiKey}`,
                "Content-Type": "application/json"
            },
            body: JSON.stringify({
                message: userMsg,
                model: modelName,
                preamble: systemMsg,
                temperature: 0.7
            })
        });

        if (!response.ok) {
            const errorData = await response.json();
            throw new Error(`Cohere API Error: ${errorData.message || response.statusText}`);
        }

        const data = await response.json();
        return data.text || "";
    } catch (error) {
        throw error;
    }
};

// --- MAIN AI HANDLER ---
export const generateAIResponse = async (userMessage, context = "", projectTitle = "") => {
    try {
        // 1. Get User Preferences from LocalStorage
        let apiKey = import.meta.env.VITE_GEMINI_API_KEY || "";
        let modelName = "gemini-1.5-flash"; // Default to a stable model
        let provider = "gemini";

        try {
            const savedPrefs = localStorage.getItem('jama1_global_prefs');
            if (savedPrefs) {
                const parsed = JSON.parse(savedPrefs);

                if (parsed.userProvider) {
                    provider = parsed.userProvider;
                }

                // Priority: User Custom Key > Env Key
                if (parsed.userApiKey && parsed.userApiKey.trim().length > 5) {
                    apiKey = parsed.userApiKey.trim();
                }

                if (parsed.userModel) {
                    modelName = parsed.userModel;
                }
            }
        } catch (e) {
            console.error("Error reading local settings", e);
        }

        if (!apiKey) {
            return {
                text: "‚ö†Ô∏è Falta la API Key. Ve a 'Configuraci√≥n' y a√±ade tu clave.",
                suggestions: ["Ir a Configuraci√≥n"]
            };
        }

        console.log(`ü§ñ AI Request | Provider: ${provider} | Model: ${modelName}`);

        const systemPrompt = `
        ACT AS AN EXPERT PROJECT MANAGER.
        CONTEXT FOR PROJECT "${projectTitle}":
        ${context || "General tasks."}
        
        INSTRUCTIONS:
        1. Analyze based on context.
        2. If asking for specific tasks, list them clearly.
        3. Be direct and concise.
        `;

        let text = "";
        const messages = [
            { role: "system", content: systemPrompt },
            { role: "user", content: userMessage }
        ];

        if (provider === 'groq') {
            text = await generateGroqResponse(apiKey, modelName, messages);
        } else if (provider === 'huggingface') {
            text = await generateHuggingFaceResponse(apiKey, modelName, messages);
        } else if (provider === 'cohere') {
            text = await generateCohereResponse(apiKey, modelName, messages);
        } else {
            // --- GEMINI EXECUTION (Default) ---
            const dynamicGenAI = new GoogleGenerativeAI(apiKey);
            const model = dynamicGenAI.getGenerativeModel({ model: modelName });

            const prompt = `${systemPrompt}\n\nUSER REQUEST: "${userMessage}"`;
            const result = await model.generateContent(prompt);
            const response = await result.response;
            text = response.text();
        }

        // Simple parsing logic for suggestions (shared)
        const suggestions = text.split('\n')
            .filter(line => line.trim().match(/^[-*1-9]/))
            .map(line => line.replace(/^[-*0-9.)]+/, '').trim())
            .slice(0, 5);

        return { text, suggestions };

    } catch (error) {
        console.error("AI Configuration Error FULL:", error);

        // Default detailed error
        let errorMsg = `Error T√©cnico: ${error.message || error.toString()}`;

        // Standardized Error Messages
        const msgLow = errorMsg.toLowerCase();
        if (msgLow.includes("403") || msgLow.includes("leaked") || msgLow.includes("permission denied")) {
            errorMsg = "‚õî API Key Bloqueada o Incorrecta. Verifica tu clave en configuraci√≥n.";
        } else if (msgLow.includes("429") || msgLow.includes("quota") || msgLow.includes("rate limit") || msgLow.includes("limit")) {
            errorMsg = "‚è≥ Cuota excedida (Rate Limit). El proveedor est√° limitando las peticiones. Espera un momento o cambia de proveedor.";
        } else if (msgLow.includes("404") || msgLow.includes("not found") || msgLow.includes("decommissioned")) {
            errorMsg = "‚ùå Modelo no disponible o retirado. Selecciona otro modelo en Configuraci√≥n.";
        } else if (msgLow.includes("401") || msgLow.includes("unauthorized")) {
            errorMsg = "üîë API Key Inv√°lida (401). Verifica que sea correcta para el proveedor seleccionado.";
        } else if (msgLow.includes("loading")) {
            errorMsg = "‚è≥ El modelo se est√° cargando en el servidor (Cold Boot). Intenta de nuevo en 30s.";
        }

        return {
            text: errorMsg,
            suggestions: ["Ir a Configuraci√≥n", "Reintentar"]
        };
    }
};

export const testConnection = async (apiKey, modelName, provider = "gemini") => {
    try {
        console.log(`Testing connection | Provider: ${provider} | Key ending: ...${apiKey?.slice(-4)}`);

        if (provider === 'groq') {
            await generateGroqResponse(apiKey, modelName, [{ role: "user", content: "Hello" }]);
            return { success: true, message: "Conexi√≥n con Groq Exitosa" };
        } else if (provider === 'huggingface') {
            await generateHuggingFaceResponse(apiKey, modelName, [{ role: "user", content: "Hello" }]);
            return { success: true, message: "Conexi√≥n con Hugging Face Exitosa" };
        } else if (provider === 'cohere') {
            await generateCohereResponse(apiKey, modelName, [{ role: "user", content: "Hello" }]);
            return { success: true, message: "Conexi√≥n con Cohere Exitosa" };
        } else {
            // Gemini Test
            const genAI = new GoogleGenerativeAI(apiKey);
            const model = genAI.getGenerativeModel({ model: modelName || "gemini-1.5-flash" });
            const result = await model.generateContent("Hello");
            const response = await result.response;
            const text = response.text ? response.text() : "Success";
            return { success: true, message: "Conexi√≥n con Gemini Exitosa" };
        }

    } catch (error) {
        console.error("Test Connection Exception:", error);

        let msg = error.message || "Error desconocido";
        msg = msg.toString();
        const msgLow = msg.toLowerCase();

        if (msgLow.includes("403") || msgLow.includes("leaked")) msg = "Key Bloqueada/Filtrada (403)";
        if (msgLow.includes("429") || msgLow.includes("quota") || msgLow.includes("limit")) msg = "‚è≥ L√≠mite de Cuota Excedido (429). Espera 1 min.";
        if (msgLow.includes("401") || msgLow.includes("unauthorized") || msgLow.includes("invalid api key")) msg = "‚ùå Key Inv√°lida (401). Verifica que la copiaste bien.";
        if (msgLow.includes("404") || msgLow.includes("not found") || msgLow.includes("decommissioned")) msg = "‚ùå Modelo retirado/no existe. Cambia el modelo.";
        if (msgLow.includes("fetch failed")) msg = "Error de Red / Conexi√≥n";

        return { success: false, message: msg };
    }
};
